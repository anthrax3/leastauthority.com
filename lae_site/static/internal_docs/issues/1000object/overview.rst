
Problem Statement:
==================

.. _ticket #1678: https://tahoe-lafs.org/trac/tahoe-lafs/ticket/1678
.. _doc: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTBucketGET.html

Quoting from `ticket #1678`_:

 ``The GET Bucket AWS call may return a truncated response, by default after 1000 
 objects(``\ `doc`_\ ``). Currently we don't take that into account (actually I forgot 
 that we didn't :-( ), which might be causing some of the 410 Gone errors.``


Implemented Solution:
=====================

DavidSarah has deployed a "`prefix query patch`_" to the storage servers,
which reduces the query response keys to the set of those specified by the prefix. 

.. _prefix query patch: https://tahoe-lafs.org/trac/tahoe-lafs/changeset/5634/ticket999-S3-backend


Experimental Goals:
===================

(#) Determine whether the implemented solution is sufficient to cause up- and down-, loads to succeed in production, i.e. in the presence of buckets containing more than 1000 objects.

(#) Document the number and type of incidents in the presence and absence of the patch.

Process:
~~~~~~~~

Requests are posted from the storage server to the Amazon S3 backend.  The
requests are initiated by a client Gateway running `Tahoe`_ invoking its ``get`` functionality
repeatedly, to download a single immutable.  This behavior is similar to that of a
user repeatedly reading a single file.  

To determine the effect of the implemented solution I perform an identical
set of requests, with and without the "`prefix query patch`_". For each
set of requests I:

(1) count the number of different types of error generating actions logged on
the StorageServer

(2) describe the error types and counts under the case and control regimes.

I include a `protocol with sufficient detail for replication`_ of the experiment.

.. _protocol with sufficient detail for replication: ./expt02_howto.html
.. _Tahoe: https://tahoe-lafs.org/trac/tahoe-lafs/wiki/Installation

Observation:
~~~~~~~~~~~~

.. _the realtime monitoring howto: ../realtimemonitoring_howto.rst
.. _the repeatable protocol: ./expt02_howto.html

The process is monitorable in realtime as described in `the realtime
monitoring howto`_. Postmortem observations can be gleaned from a log file
generated by the request generating script as described in `the repeatable
protocol`_.

Results:
~~~~~~~~

Control Run:
------------

Prefix Query Patched (Case) Run:
--------------------------------

.. _trialtime: ./repeatmutablefileput.py
.. _stopt: ./repeatmutablefileput.py

* The run was initiated at (trialtime_):
* The last incident stopped at (stopt_):
* Run Length XXXXX-XXXXX : XXX (~XXX.XXX hours)
* Number of "DUMPFILEs" generated:   XXXXX


I examined the incidents that were generated during the "Case" run.
