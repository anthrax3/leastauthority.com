
Problem Statement:
==================

During some operations, at least HTTP "PUT" requests invoked in the course of
a backup, the gateway receives an error message from the storageserver.  The
storageserver is (probably) re-transmitting an error it received from the S3
instance.  The error emitted by the S3 instance "S3Error" is an HTTP response
with a 500 header, and body:

  ``"Error Message: We encountered an internal error. Please try again."``

Research:
=========

Zooko and David-Sarah performed extensive research documented in `Tahoe-LAFS
ticket 1590`_.

It is worth noting that the `AWS Service Level Agreement`_ says that AWS
intends for at least 99% of requests to succeed and less than 1% of requests
to incur errors like this, and they will pay a discount if they fail to
achieve that. It looks like we're getting the error on substantially more
than 1% of requests. This may indicate some undiagnosed problem on AWS's
side.

.. _Tahoe-LAFS ticket 1590: https://tahoe-lafs.org/trac/tahoe-lafs/ticket/1590
.. _AWS Service Level Agreement: https://aws.amazon.com/s3-sla/

Implemented Solution:
=====================

Pending better diagnosis of the issue, we have deployed a "`retry-once-patch`_"
to the storage server, which retries requests that receive "HTTP-5xx" -headed
(a superset of the "HTTP-500" -headed) responses. We retry only one time,
and if it fails a second time we return the error to the client.

.. _retry-once-patch: https://tahoe-lafs.org/trac/tahoe-lafs/browser/ticket999-S3-backend/src#allmydata

Experimental Goals:
===================

(#) Determine whether the implemented solution is sufficient to cause up- and down-, loads to succeed in production, i.e. in the presence of occasional HTTP-500 responses.

(#) Determine the distributions of HTTP Response codes Amazon S3 generates.

(#) Document the frequency of 5xx-headed HTTP Responses Amazon S3 generates. 

Process:
~~~~~~~~

.. _is linked here: ./expt01_howto.html

Requests are posted from the storage server to the Amazon S3 backend.
The requests are initiated by a client gateway invoking its ``put``
functionality repeatedly, to update a single SDMF.  This behavior is similar
to that of a user repeatedly uploading modifications to a single
file. The replicable, detailed, protocol for the process `is linked here`_.

To determine the effect of the Implemented Solution I perform an identical
set of requests with and without the "`retry-once-patch`_", and count the
number and type of HTTP Responses arriving at the StorageServer, and arriving
at the Gateway.


Observation:
~~~~~~~~~~~~

.. _realtimemonitoring_howto.rst: ../realtimemonitoring_howto.rst
.. _the repeatable protocol: ./expt01_howto.html

The process is monitorable in realtime as described in
realtimemonitoring_howto.rst_. Postmortem observations can be gleaned from a
log file generated by the request generating script as described in `the
repeatable protocol`_.
 


Control Run:
------------

.. _1336444373.21.txt: ./1336444373.21.txt

The 10,000 iterations of "``tahoe put WRITECAPURI``", against a storage server
that did *not* have the 5xx-retry patch, began at about
``1336444373.21``. The test completed at about ``1336469316.12``
approximately 6.929 hours later [#]_.

All non-200 HTTP codes included in response headers to the Gateway were
recorded with a timestamp in a local file: "`1336444373.21.txt`_" .  Of the
10,000 ``put`` 's, 133 resulted in a response at the Gateway with an HTTP
Header Code other than 200.  Of those 69 were "500 Internal Server Error"'s,
63 were "Error: 410 Gone"'s, and one was without a standard error message.
The last "Codeless" error is on line 166 of `1336444373.21.txt`_.

Patched Run:
------------

.. _1336506965.38.txt: ./1336506965.38.txt

The 10,000 iterations of "``tahoe put WRITECAPURI``", against a storage
server that **did** have the 5xx-retry patch, began at about
``1336506965.38``. The test completed at about ``1336522190`` [#]_ approximately
4.229 hours later [#]_.

All non-200 HTTP codes included in response headers to the Gateway were
recorded with a timestamp in a local file: "`1336506965.38.txt`_" .  Of the
10,000 ``put`` 's, 9 resulted in a response at the Gateway with an HTTP
Header Code other than 200.  Of those 2 were "500 Internal Server Error"'s, and
7 were "Error: 410 Gone"'s.



.. [#] The following was used to interpolate from the last error time to the total experiment runtime: 

 ``python -c 'print ((1336469042.29-1336444373.21)  * (1.0+((10000-9889.)/10000.)))/3600.'``

.. [#] The following was used to check that the ``stat`` reported mtime was plausibly the end of the test run:
 ::

  0 b: /home/arc/LAEWork/tgscripts/mutablefiletest$ python -c 'print (((1336521935.71-1336506965.38) + ((1336521935.71-1336506965.38)*(10000.-9829.)/10000.)))/3600.'
  4.22953406748

 Since the total time run based on the above linear extrapolation is within 5
 10000ths of an hour of the mtime reported by ``stat`` I am reassured that
 the reported mtime is not due to a spurious operation on the file, but
 rather the last call to file-handle close, performed by the test. 

.. [#] The following was used to compute total expt time for the "with-retries" trial: 
 ::

  0 b: /home/arc/LAEWork/tgscripts/mutablefiletest$ stat --format=%Y 1336506965.38.txt
  1336522190
  0 b: /home/arc/LAEWork/tgscripts/mutablefiletest$ python -c 'print (1336522190-1336506965.38)/3600.'
  4.22906111108

